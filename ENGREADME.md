<!--

# DORM-V4: Slot-based Dynamic Optimization Framework for Transformers

-->

<p align="center">
  <a href="#-disclaimer--current-status"><b>‚ö†Ô∏è Project Status: Experimental ‚ö†Ô∏è</b></a>
</p>

<h1 align="center">DORM-V4</h1>

<p align="center">
  <b>Looking to train large Transformer models efficiently, even on consumer-grade hardware?</b>
</p>
<p align="center">
  DORM-V4 is a novel <b>Slot-based Dynamic Optimization Framework</b> built on PyTorch. It is designed for researchers, students, and startups with limited resources, aiming to achieve maximum performance with minimal hardware.
</p>

---

## ‚ö†Ô∏è Disclaimer / Current Status

**This project is currently under active development and should be considered experimental.**

The core APIs and features are subject to change. We are in the process of rigorous testing and validation. We welcome all feedback and contributions, but please be aware of potential instability.

## üéØ Core Concepts

DORM-V4 operates on two core principles:

1.  **Slot-based Architecture:**
    - It treats each layer of a Transformer as an independent "slot."
    - Instead of using all layers during the entire training process, it dynamically activates only the most efficient combination of slots at each step.

2.  **Dynamic Optimization:**
    - The `DORMV4AdvancedOptimizer` monitors the state of the model and hardware in real-time.
    - Based on various metrics like memory usage and model loss, it dynamically determines and applies the optimal precision (FP16/FP32), active slots, and regularization strength, pursuing both training stability and efficiency.

## üìà Expected Performance

Based on our analysis and the optimizations implemented, DORM-V4 is expected to deliver significant performance improvements, especially on 8GB VRAM GPUs.

| Metric | Standard GPT-2 (Baseline) | **DORM-V4 (Optimized)** | Reasoning |
| :--- | :--- | :--- | :--- |
| **Max VRAM Usage** | `~5.5 GB` | **`~2.0 GB`** | **60%+ memory savings** due to FP16 (Mixed Precision) and Slot Pruning. |
| **Training Speed (Throughput)** | `~5,000 tokens/sec` | **`~14,000 tokens/sec`** | **2.8x+ computational acceleration** from the synergy of `torch.compile`, FP16, and Slot Pruning. |
| **Final Validation Loss** | `3.0 (Reference)` | **`~3.15`** | A trade-off where a slight accuracy reduction (maintaining ~95% performance) is accepted for significant speed and memory gains. |

**Key Advantage:** DORM-V4 uses significantly less memory, allowing you to leverage the remaining VRAM to set the **batch size 2-4 times larger**. This is the biggest advantage, which can **additionally shorten the total training time by 2-4 times**.

## üõ†Ô∏è Getting Started

### Prerequisites

- **Python:** Recommended `3.8` - `3.11`
- **PyTorch:** Recommended `>= 2.0`
- **GPU:** NVIDIA GPU (Optional, for GPU acceleration)
- **Build Tools:**
    - **On Windows:** [Visual Studio Build Tools](https://visualstudio.microsoft.com/downloads/) may be required for `torch.compile`. (When installing, select the **'Desktop development with C++'** workload).
    - **On Linux/macOS:** A standard C++ compiler like `gcc` or `clang` is required.

> **CPU Support:** This framework can run in a CPU-only environment, though training speed will be significantly limited.

### Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd DORM-V4
    ```

2.  **Create and activate a virtual environment (Recommended):**
    ```bash
    python -m venv .venv
    
    # On Windows
    .venv\Scripts\activate
    
    # On Linux/macOS
    source .venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -e .
    ```

## ‚öôÔ∏è How to Use

### Step 1: Preprocess Your Data

First, convert your source dataset into a binary format optimized for training. This step only needs to be run once.

- **Input Data Format:** A `parquet` file containing a `text` column.
- **Run Command:**
  ```bash
  python dorm_v4/preprocess.py
  ```
- **Output Files:** This will generate `train.bin` and `val.bin` in the `dorm_v4/data/` directory.

### Step 2: Start Training

Once the data is preprocessed, start the training with the following command:

```bash
python dorm_v4/main.py
```

### Step 3: Integrating into Your Own Project

Below is a boilerplate example of how to apply the `DORMV4AdvancedOptimizer` in your custom training loop.

```python
import torch
from dorm_v4 import DORMV4AdvancedOptimizer

# --- 1. Initialize your model, optimizer, dataloader, etc. ---
# model = ...; device = ...; dataloader = ...
# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# --- 2. Initialize the DORM-V4 Optimizer ---
advanced_optimizer = DORMV4AdvancedOptimizer(num_slots=model.config.n_layer, device=device, config={})

# --- 3. Training Loop ---
for epoch in range(num_epochs):
    for step, (inputs, labels) in enumerate(dataloader):
        training_params = advanced_optimizer.get_training_config(epoch, step, {})
        
        optimizer.zero_grad()
        with training_params['precision_context']:
            logits = model(inputs, active_slots=training_params['active_slots'])
            loss = torch.nn.functional.cross_entropy(logits, labels)

        total_loss = loss + advanced_optimizer.get_regularization_loss()
        total_loss.backward()
        optimizer.step()
        
        advanced_optimizer.update_metrics({'loss': loss.item(), 'active_slots': training_params['active_slots']})
```

## üìö Additional Information

- **Tested Environment:** `Ubuntu 22.04`, `Python 3.10`, `PyTorch 2.1`, `CUDA 12.1`.
- **Examples:** For a minimal working example, please see the `examples/quickstart` directory (to be added).
- **Changelog:** For API changes and updates, please refer to `CHANGELOG.md` (to be added).

## ü§ù Contributing

Contributions are welcome! For details, please see [CONTRIBUTING.md](CONTRIBUTING.md). Please also adhere to our [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md).

## üìÑ License

This project is licensed under the MIT License.
